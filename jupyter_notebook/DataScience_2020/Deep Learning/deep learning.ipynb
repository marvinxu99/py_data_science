{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow <br>\n",
    "MNIST <br>\n",
    "Customer Acquisition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are three types of machine learning\n",
    "- <b>Supervised</b>: it is provided with inputs and desired outputs, the goal is to produce results close to targeted output <br> \n",
    "- <b>Unsupervised</b>: we feed input, the machine to find patterns. No target outputs <br>\n",
    "- <b>Reinforcement</b>: give rewards when result is correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training am algorithm involves 4 ingredients:\n",
    "Data <br>\n",
    "Model <br>\n",
    "Objective Function <br>\n",
    "Optimization Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <b>training process</b> is essentially a trial-and-error process, but each consequent trial is better than the previous one, as we have methods in place that give feedback to the algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Model:\n",
    "    y = xw + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array([1.2,-3])\n",
    "x = np.array([2,3])\n",
    "b = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40000000000000036"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.dot(x, w) + b\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective functions:\n",
    "- Loss function - for supervised learning, function we are trying to minimize <br>\n",
    "- Reward function - reinforcement learning, functions we are trying to maximize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss functions:\n",
    "supervised learnig:\n",
    "    - Regression: L2-norm <br>\n",
    "    - Classification: Cross-entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"L2-norm.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cross-entropy = L(y,t) = - sume of (ti * ln(yi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent:\n",
    "1-parameter Gradient Descent\n",
    "2-parameter Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient descent is not the only way to optimize an algorithm. However, it is the most basic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A too high learning rate may cause the loss function to diverge to infinity, instead of finding the minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
